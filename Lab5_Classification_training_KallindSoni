{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport transformers\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import model_selection, metrics","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-08-24T17:00:30.786191Z","iopub.execute_input":"2024-08-24T17:00:30.786862Z","iopub.status.idle":"2024-08-24T17:00:35.627766Z","shell.execute_reply.started":"2024-08-24T17:00:30.786827Z","shell.execute_reply":"2024-08-24T17:00:35.626768Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:00:57.349341Z","iopub.execute_input":"2024-08-24T17:00:57.350186Z","iopub.status.idle":"2024-08-24T17:00:57.361519Z","shell.execute_reply.started":"2024-08-24T17:00:57.350146Z","shell.execute_reply":"2024-08-24T17:00:57.360634Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n* Dataset class\n* Model\n* Trainer - training arguments\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:00:59.930219Z","iopub.execute_input":"2024-08-24T17:00:59.931192Z","iopub.status.idle":"2024-08-24T17:00:59.938316Z","shell.execute_reply.started":"2024-08-24T17:00:59.931140Z","shell.execute_reply":"2024-08-24T17:00:59.937480Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'\\n* Dataset class\\n* Model\\n* Trainer - training arguments\\n'"},"metadata":{}}]},{"cell_type":"code","source":"config = {\n    \"max_length\": 360,\n    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n    \n    \"output_dir\": \"./my-model\",\n    \"train_batch_size\": 64,\n    \"valid_batch_size\": 64,\n    \"learning_rate\": 3e-5,\n    \"epochs\": 3,\n    \n    \"debug\": True,\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:01:00.244530Z","iopub.execute_input":"2024-08-24T17:01:00.244842Z","iopub.status.idle":"2024-08-24T17:01:00.249491Z","shell.execute_reply.started":"2024-08-24T17:01:00.244810Z","shell.execute_reply":"2024-08-24T17:01:00.248598Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])\nclass TextDataset:\n    \n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return self.data.shape[0]\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        enc = tokenizer(\n            row[\"text\"],\n            add_special_tokens=True,\n            max_length=config[\"max_length\"],\n            padding=\"max_length\",\n            truncation=True\n        )\n        \n        return {\n            \"input_ids\": torch.tensor(enc[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(enc[\"attention_mask\"]),\n            \"labels\": torch.tensor(row[\"label\"]),\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:01:00.771038Z","iopub.execute_input":"2024-08-24T17:01:00.771394Z","iopub.status.idle":"2024-08-24T17:01:02.200425Z","shell.execute_reply.started":"2024-08-24T17:01:00.771359Z","shell.execute_reply":"2024-08-24T17:01:02.199533Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f458df4f4c5a4a8a9aa1793a9364f497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cffb43ee7bd485bbb41fcb467bb0e39"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/IMDB Dataset.csv\").rename(columns={\"review\": \"text\"})\n\nid2label = {0: \"negative\", 1: \"positive\"}\nlabel2id = {label: id_ for id_, label in id2label.items()}\n\ndf[\"label\"] = df[\"sentiment\"].map(label2id)\n\n\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:55.390034Z","iopub.execute_input":"2024-08-24T17:11:55.390908Z","iopub.status.idle":"2024-08-24T17:11:56.028595Z","shell.execute_reply.started":"2024-08-24T17:11:55.390863Z","shell.execute_reply":"2024-08-24T17:11:56.027672Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"(50000, 3)\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                                text sentiment  label\n0  One of the other reviewers has mentioned that ...  positive      1\n1  A wonderful little production. <br /><br />The...  positive      1\n2  I thought this was a wonderful way to spend ti...  positive      1\n3  Basically there's a family where a little boy ...  negative      0\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:56.030160Z","iopub.execute_input":"2024-08-24T17:11:56.030489Z","iopub.status.idle":"2024-08-24T17:11:56.612404Z","shell.execute_reply.started":"2024-08-24T17:11:56.030456Z","shell.execute_reply":"2024-08-24T17:11:56.611425Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train, valid = model_selection.train_test_split(\n    df,\n    test_size=0.2,\n    random_state=23,\n    shuffle=True,\n    stratify=df[\"label\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:56.613554Z","iopub.execute_input":"2024-08-24T17:11:56.613847Z","iopub.status.idle":"2024-08-24T17:11:56.641153Z","shell.execute_reply.started":"2024-08-24T17:11:56.613815Z","shell.execute_reply":"2024-08-24T17:11:56.640274Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_ds = TextDataset(train)\nvalid_ds = TextDataset(valid)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:56.643148Z","iopub.execute_input":"2024-08-24T17:11:56.643478Z","iopub.status.idle":"2024-08-24T17:11:56.647671Z","shell.execute_reply.started":"2024-08-24T17:11:56.643445Z","shell.execute_reply":"2024-08-24T17:11:56.646751Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"valid_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:56.648657Z","iopub.execute_input":"2024-08-24T17:11:56.648915Z","iopub.status.idle":"2024-08-24T17:11:56.665996Z","shell.execute_reply.started":"2024-08-24T17:11:56.648886Z","shell.execute_reply":"2024-08-24T17:11:56.665147Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  1045,  2079,  2131, 15560,  2007,  2715, 17241,  1997,  8101,\n          2043,  1996,  2472,  2064,  1005,  1056,  2191,  2010,  2568,  2039,\n          3251,  2000,  2224,  1996,  2434,  2030,  2000, 10651,  2009,  1012,\n          2065,  2009,  1005,  1055,  2478,  1996,  2434,  2616,  1999,  2019,\n          7172,  4292,  1010,  2008,  1005,  1055,  3391, 24026,  2065,  2275,\n          1999,  1996,  3983,  2030,  7398,  2301,  2348,  2009,  2064,  2147,\n          7929,  1999,  2558,  6782,  1010,  1041,  2290,  1996,  8672, 16634,\n          2078, 11313,  2305,  2275,  2397,  6652,  2200,  6464,  1012,  2009,\n          2071,  2147,  2007,  1996,  2382,  1005,  1055,  4292,  2065,  2069,\n          2045,  2018,  2042,  2521,  2625,  1997,  1996,  2299,  1998,  3153,\n          1998,  2521,  2062,  1997,  8101,  1005,  1055,  3793,  1012,  6854,\n          1010,  2009,  2074,  4515,  2039,  2108,  1037,  3492, 20610,  2295,\n          2200,  8242,  2265,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n          1013,  1028,  2178,  3291,  2003, 24905, 17988,  2370,  1012,  1045,\n          5993,  2002,  1005,  1055,  2521,  2205,  2214,  2000,  2377,  2028,\n          1997,  1996,  2493,  2021,  2062,  2590,  1010,  2002,  1005,  1055,\n          2107,  2019,  5281,  8101,  2319,  3364,  2008,  1999,  8741,  1997,\n          2035,  2010,  4073,  2000,  2022,  2074,  2178,  3076,  1010,  2010,\n          3997,  1997,  3772,  3065,  2035,  1996,  2051,  1012,  1997,  2607,\n          2002,  2323,  2031,  2209,  1996,  2332,  1011,  2053,  3291,  1999,\n          2383,  1037,  9677,  3076,  2332,  5129,  2011,  3920,  2493,  1012,\n          2612,  2057,  2018,  1037,  8242,  2021,  4895,  5714,  6873,  7741,\n          3364,  2005,  1996,  2332,  1010,  2947,  2019,  4895,  5714,  6873,\n          7741,  2061,  1011,  2170,  2332,  2007,  2053,  2332,  2135, 12332,\n          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n          3815,  1997,  2299,  1998,  3153,  1010,  2029,  1045,  2179,  6945,\n          6313,  1999,  8741,  1997,  1996,  3835,  2774,  1998,  8242,  2438,\n          5613,  1010,  6854,  3214,  1996,  2307,  8101,  2319,  7982,  2018,\n          2000,  2022,  3013,  2091, 21040,  1012,  2061,  1996,  2878,  2518,\n          4515,  2039,  1037, 20610,  1998, 10256,  9530, 25969,  3258,  1010,\n          1998,  1045,  2288,  2200, 11471,  1010,  2164,  2007,  1996,  5021,\n          4332,  1010,  1998,  2001,  5580,  2043,  2009,  3092,  1012, 24905,\n         17988,  2038,  2025,  2589,  8101,  3425,  1999,  2023,  2537,  1012,\n          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028, 27447,  2174,\n          2000,  2957,  7987, 10136,  1998, 26987, 11338,  7974,  2319,  1010,\n          7078, 21459,  2004,  1996,  3080,  3232,  1012,   102,     0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]),\n 'labels': tensor(0)}"},"metadata":{}}]},{"cell_type":"code","source":"model = transformers.AutoModelForSequenceClassification.from_pretrained(config[\"model_path\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:11:56.666917Z","iopub.execute_input":"2024-08-24T17:11:56.667167Z","iopub.status.idle":"2024-08-24T17:11:56.971996Z","shell.execute_reply.started":"2024-08-24T17:11:56.667139Z","shell.execute_reply":"2024-08-24T17:11:56.971277Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef compute_metrics(eval_data):\n   \n    preds = eval_data.predictions.argmax(-1)\n    labels = eval_data.label_ids \n    print(eval_data)\n    print(preds)\n    print(labels)\n\n    return {\n        'accuracy': metrics.accuracy_score(labels, preds),\n        'precision': metrics.precision_score(labels, preds),\n        'recall': metrics.recall_score(labels, preds),\n        'classification_report': metrics.classification_report(labels, preds, target_names=list(id2label.values()), output_dict=True)\n\n\n\n\n    }\n\ntraining_args = transformers.TrainingArguments(\n     output_dir=\"./results\",                      # Directory for storing results\n    evaluation_strategy=\"steps\",                 # Evaluate every few steps\n    per_device_train_batch_size=config['train_batch_size'],              # Batch size per device during training\n    per_device_eval_batch_size=config['train_batch_size'],               # Batch size per device during evaluation\n    num_train_epochs=config['epochs'],                          # Total number of training epochs\n    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n    save_total_limit=2,\n    logging_dir=None,                            # Disable logging directory\n    logging_strategy=\"no\",\n    report_to=[]# Limit the total amount of checkpoints`\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:26:06.921892Z","iopub.execute_input":"2024-08-24T17:26:06.922857Z","iopub.status.idle":"2024-08-24T17:26:06.954515Z","shell.execute_reply.started":"2024-08-24T17:26:06.922813Z","shell.execute_reply":"2024-08-24T17:26:06.953579Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,                                 # The model to be trained\n    args=training_args,                          # The training arguments, defined above\n    train_dataset=train_ds,                 # The training dataset\n    eval_dataset=valid_ds,                   # The evaluation dataset\n    tokenizer=tokenizer,                         # The tokenizer\n    compute_metrics=compute_metrics, \n    \n\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:26:09.471984Z","iopub.execute_input":"2024-08-24T17:26:09.472696Z","iopub.status.idle":"2024-08-24T17:26:09.485806Z","shell.execute_reply.started":"2024-08-24T17:26:09.472653Z","shell.execute_reply":"2024-08-24T17:26:09.484812Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:26:12.031202Z","iopub.execute_input":"2024-08-24T17:26:12.032080Z","iopub.status.idle":"2024-08-24T17:39:26.131874Z","shell.execute_reply.started":"2024-08-24T17:26:12.032038Z","shell.execute_reply":"2024-08-24T17:39:26.130986Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 13:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Classification Report</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>No log</td>\n      <td>0.250403</td>\n      <td>0.902600</td>\n      <td>0.924863</td>\n      <td>0.876400</td>\n      <td>{'negative': {'precision': 0.8825541619156214, 'recall': 0.9288, 'f1-score': 0.9050867277333853, 'support': 5000}, 'positive': {'precision': 0.9248628113127902, 'recall': 0.8764, 'f1-score': 0.8999794619018279, 'support': 5000}, 'accuracy': 0.9026, 'macro avg': {'precision': 0.9037084866142058, 'recall': 0.9026, 'f1-score': 0.9025330948176066, 'support': 10000}, 'weighted avg': {'precision': 0.9037084866142059, 'recall': 0.9026, 'f1-score': 0.9025330948176066, 'support': 10000}}</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>No log</td>\n      <td>0.234584</td>\n      <td>0.915200</td>\n      <td>0.932680</td>\n      <td>0.895000</td>\n      <td>{'negative': {'precision': 0.8990772779700116, 'recall': 0.9354, 'f1-score': 0.9168790433248383, 'support': 5000}, 'positive': {'precision': 0.932680283451438, 'recall': 0.895, 'f1-score': 0.9134517248418045, 'support': 5000}, 'accuracy': 0.9152, 'macro avg': {'precision': 0.9158787807107248, 'recall': 0.9152, 'f1-score': 0.9151653840833214, 'support': 10000}, 'weighted avg': {'precision': 0.9158787807107248, 'recall': 0.9152, 'f1-score': 0.9151653840833213, 'support': 10000}}</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>No log</td>\n      <td>0.227588</td>\n      <td>0.919800</td>\n      <td>0.934936</td>\n      <td>0.902400</td>\n      <td>{'negative': {'precision': 0.9056822574410515, 'recall': 0.9372, 'f1-score': 0.9211716139178298, 'support': 5000}, 'positive': {'precision': 0.9349357646083714, 'recall': 0.9024, 'f1-score': 0.9183798086708733, 'support': 5000}, 'accuracy': 0.9198, 'macro avg': {'precision': 0.9203090110247114, 'recall': 0.9198, 'f1-score': 0.9197757112943515, 'support': 10000}, 'weighted avg': {'precision': 0.9203090110247114, 'recall': 0.9198, 'f1-score': 0.9197757112943516, 'support': 10000}}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1875, training_loss=0.20091139322916668, metrics={'train_runtime': 793.6161, 'train_samples_per_second': 151.207, 'train_steps_per_second': 2.363, 'total_flos': 1245553977600000.0, 'train_loss': 0.20091139322916668, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:39:57.722753Z","iopub.execute_input":"2024-08-24T17:39:57.723469Z","iopub.status.idle":"2024-08-24T17:39:57.729926Z","shell.execute_reply.started":"2024-08-24T17:39:57.723425Z","shell.execute_reply":"2024-08-24T17:39:57.728922Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:40:00.511962Z","iopub.execute_input":"2024-08-24T17:40:00.512725Z","iopub.status.idle":"2024-08-24T17:40:00.649100Z","shell.execute_reply.started":"2024-08-24T17:40:00.512685Z","shell.execute_reply":"2024-08-24T17:40:00.648181Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}